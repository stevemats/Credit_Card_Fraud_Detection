{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#setting style to seaborn\n",
    "sns.set_style(\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data https://www.kaggle.com/mlg-ulb/creditcardfraud?select=creditcard.csv\n",
    "# alternate https://www.dropbox.com/s/b44o3t3ehmnx2b7/creditcard.csv?dl=1\n",
    "\n",
    "#importing the data to be used in the analysis and split it into training and test data.\n",
    "file_path = \"https://www.dropbox.com/s/b44o3t3ehmnx2b7/creditcard.csv?dl=1\"\n",
    "\n",
    "# importing the dataset to a dataframe\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"Original Dataset dimensions:\", df.shape)\n",
    "\n",
    "#splitting test data\n",
    "test = df.sample(frac=0.15, random_state=0)\n",
    "df = df.drop(test.index)\n",
    "\n",
    "print(\"Train data dimensions: \", df.shape)\n",
    "print(\"Test data dimensions: \", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744002b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the first entries for the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the statistical summary for the dataset\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "Time = Number of seconds elapsed between this transaction and the first transaction in the dataset\n",
    "Amount = Transaction amount\n",
    "\n",
    "Class:\n",
    "- Grouped into two segments:\n",
    "0 and 1 as transaction type indicators.\\\n",
    "\n",
    "0 - Normal Transaction\n",
    "1 - Fraudulent Transacti\n",
    "\"\"\"\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b59b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the column with the most null entries\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the number of entries per different classes\n",
    "print(df.Class.value_counts())\n",
    "\n",
    "print(\"\\nFrauds represents {}% of the dataset \\n\".format(round(df[df.Class == 1].shape[0]*100/df.shape[0],2)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "sns.countplot(x=\"Class\", data=df, ax=ax)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to enhance the unbalanced dataset(enhancment)\n",
    "# checking the distribution for the variable Time in normal and fraudulent transactions:\n",
    "\n",
    "\n",
    "#Distirbution for \"Time\" per class\n",
    "\n",
    "n_bins=40\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(15,7))\n",
    "\n",
    "df[df.Class == 0].Time.hist(ax=ax1, bins=n_bins, grid=False)\n",
    "ax1.set_title(\"Normal\")\n",
    "\n",
    "df[df.Class == 1].Time.hist(ax=ax2, bins=n_bins, grid=False)\n",
    "ax2.set_title(\"Fraudulent\")\n",
    "ax2.set_xlabel(\"Time(Seconds)\")\n",
    "ax2.set_ylabel(\"# of Entries\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The difference in the number of entries may be explained by the different periods of the day, as day and night,\n",
    "when the number of transactions vastly differs.\n",
    "\n",
    "Plotting a boxplot for the Amount variable in normal and fraudulent transactions:\n",
    "\"\"\"\n",
    "\n",
    "#Calculating the superior limit for Amount Variable\n",
    "q3 = df[df.Class == 1].Amount.quantile(.75)\n",
    "q1 = df[df.Class == 1].Amount.quantile(.25)\n",
    "IQR = q3 - q1\n",
    "\n",
    "sup_limit = q3 + 1.5*IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe276e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plottig the boxplot for the the normal and fraudulent distibution\n",
    "fig, ax = plt.subplots(figsize=(7,10))\n",
    "sns.boxplot(x=\"Class\", y=\"Amount\", data=df, ax=ax, showmeans=True)\n",
    "ax.set_ylim(-20, sup_limit)\n",
    "ax.set_xticklabels([\"Normal\", \"Fraudulent\"])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Although the median is lower for the fraudulent transactions (represented by the black line inside each box),\n",
    "the mean (represented by the green triangle) is higher for fraudulent transactions than for normal ones.\n",
    "\n",
    "We can also plot a density plot for each variable, separating fraudulent and normal transactions. \n",
    "Here, we are searching for variables that are significantly different for normal and fraudulent transactions:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#plotting the density plot\n",
    "\n",
    "columns_names = df.drop(labels=[\"Class\"], axis=1).columns\n",
    "\n",
    "df_normal = df[df.Class == 0]\n",
    "df_fraud = df[df.Class == 1]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5,ncols=6, figsize=(18,18))\n",
    "fig.subplots_adjust(hspace=1, wspace=1)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for col in columns_names:\n",
    "  idx+=1\n",
    "  plt.subplot(5,6,idx)\n",
    "  sns.kdeplot(df_normal[col], label = \"Normal\", shade=True)\n",
    "  sns.kdeplot(df_fraud[col], label = \"Fraud\", shade=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Some variables as 'V14' and 'V4' have pretty different behavior for the two classes.\n",
    "\n",
    "After the initial exploratory analysis we can state that:\n",
    "\n",
    "    (1)The variables Time and Amount are not normalized and will need to be transformed before training the model.\n",
    "    (2)The dataset is extremely unbalanced, representing a challenge for the analysis.\n",
    "    (3)The mean amount for fraudulent transactions is higher than the normal transaction mean amount.\n",
    "    (4)Some variables, as V14 and V4, have a clear different behavior for normal and fraudulent transactions.\n",
    "\n",
    "Based on that, we can now prepare the data before training the model.\n",
    "\n",
    "==> Preparing the data <==\n",
    "\n",
    "First, we will normalize the Time and Amount variables. Since their dimensions are different from all the other variables, our model will be biased by these columns if we don't normalize them.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#normalizing \"Amount\" and \"Time\" variables\n",
    "df_copy = df.copy()\n",
    "\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "df_copy[\"std_amount\"] = std_scaler.fit_transform(df_copy.Amount.values.reshape(-1,1))\n",
    "df_copy[\"std_time\"] = std_scaler.fit_transform(df_copy.Time.values.reshape(-1,1))\n",
    "\n",
    "df_copy.drop([\"Time\",\"Amount\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the first entries\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into train and validation\n",
    "np.random.seed(2)\n",
    "X = df_copy.drop(\"Class\", axis=1)\n",
    "y = df_copy[\"Class\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Last but not least, since the fraudulent transaction only accounts for 0,17% of the dataset,\n",
    "we should balance the dataset to have better results with our models.\n",
    "\n",
    "Among others, there are two ways in which we can solve this problem:\n",
    "\n",
    "    1. Over Sampling - Creates new entries for the minority class based on the existing samples.\n",
    "    2. Under Sampling - Randomly deletes entries for the majority class.\n",
    "\n",
    "Here we will choose the under sampling method and apply it to the data:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Balancing the dataset\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ba943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting balanced values\n",
    "print(pd.Series(y_rus).value_counts())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "sns.countplot(x=y_rus, ax=ax)\n",
    "ax.set_xticklabels(labels=[\"Normal\",\"Fraudulent\"])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c041b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Now we have the same number for fraudulent and normal transactions. To better understand\n",
    "the influence of unbalanced data, letÂ´s plot a correlation matrix for the balanced and unbalanced dataset:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#plotting the correlation matrix for unbalanced and balanced data\n",
    "\n",
    "imb_corr = X_train.corr()\n",
    "corr = pd.DataFrame(X_rus).corr()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(18,8))\n",
    "fig.suptitle(\"Correlation Matrix\")\n",
    "\n",
    "sns.heatmap(corr, ax=ax[1], cmap=\"coolwarm\", linewidths=.1,\n",
    "            xticklabels=imb_corr.columns, yticklabels=imb_corr.columns)\n",
    "ax[1].set_title(\"Balanced Data\")\n",
    "\n",
    "sns.heatmap(imb_corr, ax=ax[0], cmap=\"coolwarm\", linewidth=.1)\n",
    "ax[0].set_title(\"Unbalanced Data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Training two models that use different algorithms to classify\n",
    "the data: Logistic Regression and Decision Tree Classifier. Afterward, trying to analyze which\n",
    "model better classifies news transactions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Building the first model using Logisitic Regression\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "lr_model.fit(X_rus, y_rus)\n",
    "\n",
    "y_pred = lr_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98412221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Metrics for evaluating classification models are:\n",
    "\n",
    "->Precision - the proportion of predicted Positives that are truly Positive\n",
    "->Recall - the proportion of actual positives correctly classified\n",
    "->f1-score - the harmonic mean of precision and recall\n",
    "->accuracy - the proportion of true results among the total number of cases examined\n",
    "->ROC score - indicates how well the probabilities from the positive classes are separated\n",
    "  from the negative classes\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "#Checking the metrics for the first model\n",
    "print(\"Classification Report for Logisitic Regression Model: \\n\\n\", classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "#ROC\n",
    "print(\"ROC Curve: \\n\\n\", round(roc_auc_score(y_val, y_pred),4), \"\\n\")\n",
    "\n",
    "#plotting the confusion matrix\n",
    "skplt.metrics.plot_confusion_matrix(y_val, y_pred, normalize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building other model using Decision Tree Classifier\n",
    "\n",
    "tree_depth = 4\n",
    "\n",
    "dt_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=tree_depth)\n",
    "\n",
    "dt_model.fit(X_rus, y_rus)\n",
    "\n",
    "y_pred = dt_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the Decision Tree\n",
    "\n",
    "# creating the dot\n",
    "dot = export_graphviz(dt_model, filled=True, rounded=True, feature_names=X.columns, class_names=[\"Normal\", \"Fraudulent\"])\n",
    "\n",
    "#plotting\n",
    "graph = pydotplus.graph_from_dot_data(dot)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the metrics for the second model\n",
    "print(\"Classification Report for the Decision Tree Classifier: \\n\\n\", classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "#ROC\n",
    "print(\"ROC Curve: \\n\\n\", round(roc_auc_score(y_val, y_pred),4), \"\\n\")\n",
    "\n",
    "#plotting the confusion matrix\n",
    "skplt.metrics.plot_confusion_matrix(y_val, y_pred, normalize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Model validation:\n",
    "\n",
    "We should check the metrics against data that the model has not seen before to validate it.\n",
    "We will do that using the test data.\n",
    "\n",
    "Let's first normalize the variables Time and Amount at the test data:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# normalizing test data\n",
    "\n",
    "test_copy = test.copy()\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "test_copy[\"std_amount\"] = std_scaler.fit_transform(test_copy.Amount.values.reshape(-1,1))\n",
    "test_copy[\"std_time\"] = std_scaler.fit_transform(test_copy.Time.values.reshape(-1,1))\n",
    "\n",
    "test_copy.drop([\"Amount\", \"Time\"], axis=1, inplace=True)\n",
    "\n",
    "test_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33435854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data in X and y\n",
    "X_test = test_copy.drop([\"Class\"],axis=1)\n",
    "y_test = test_copy[\"Class\"]\n",
    "\n",
    "#Prediciting test data for Logisitic Regression model\n",
    "y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563500a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Finally, let's check the accuracy and confusion matrix for the logistic regression model:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Checking the metrics for the first model\n",
    "print(\"Classification Report for Logisitic Regression Model: \\n\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "#ROC\n",
    "print(\"ROC Curve: \\n\\n\", round(roc_auc_score(y_test, y_pred),4), \"\\n\")\n",
    "\n",
    "#plotting the confusion matrix\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c775c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediciting data for the Decision Tree Model\n",
    "y_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdfc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the metrics for the second model\n",
    "print(\"Classification Report for the Decision Tree Classifier: \\n\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "#ROC\n",
    "print(\"ROC Curve: \\n\\n\", round(roc_auc_score(y_test, y_pred),4), \"\\n\")\n",
    "\n",
    "#plotting the confusion matrix\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
